import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import math
from statsmodels.stats import diagnostic
import statsmodels.stats as st
from scipy import stats
from itertools import combinations
import networkx as nx
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from pandas.plotting import scatter_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, auc, roc_curve
import missingno as msno
import ast
from collections import Counter
import statsmodels.regression.recursive_ls as rls
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pyvis.network import Network
from director_scrap import director_scrap

%load_ext autoreload
%autoreload 2








df_crew = pd.read_table('data/crew.tsv')
df_crew.head(1)








df_name = pd.read_table('data/name.tsv')
df_name.head(1)








df_title = pd.read_table('data/title.tsv')
df_title.head(1)








df_rating = pd.read_table('data/imdb_rating.tsv')
df_rating.head(1)








df_principals = pd.read_table('data/principals.tsv')
df_principals.head(1)








#Drop useless informations
df_p = df_principals.drop(['job', 'characters', 'ordering'], axis=1)

#Keep only one person per role
df_p_drop = df_p.drop_duplicates(subset=['tconst', 'category'])

#Pivot the dataframe so we have the each role in column format for a given movie
df_pp = df_p_drop.pivot(index='tconst', columns='category', values='nconst').reset_index()

#Keep only composer and producer
df_roles = df_pp[['tconst', 'composer', 'producer']]

#Retrieve the names for the roles via the 'Name' database
df_co = df_roles.merge(df_name, left_on='composer', right_on='nconst', how='inner')
df_co = df_co[['tconst', 'producer', 'primaryName']].rename(columns={'primaryName': 'Composer'})
df_prod = df_co.merge(df_name, left_on='producer', right_on='nconst', how='inner')
df_prod = df_prod[['tconst', 'primaryName', 'Composer']].rename(columns={'primaryName': 'Producer'})

#Print the result
df_prod.head(1)


#Associate movie rating with crew
df_crew_rating = df_crew.merge(df_rating, how='inner', on='tconst')
df_cr = df_crew_rating.drop(['numVotes'], axis=1)

#Associate the name of the movie to the rest
df_cr_title = df_cr.merge(df_title[['tconst', 'primaryTitle', 'startYear']], how='inner', on='tconst')

#Add the composer and producer
df_t = df_cr_title.merge(df_prod, on='tconst', how='left')

#Add the director
df_crt_dir = df_t.merge(df_name[['nconst', 'primaryName']], left_on='directors', right_on='nconst', how='inner')

#Add the writer
df_fin = df_crt_dir.merge(df_name[['nconst', 'primaryName']], left_on='writers', right_on='nconst', how='left')

df_fin.head(1)





#Drop useless columns
df_dropped = df_fin.drop(['tconst', 'directors', 'writers', 'nconst_x', 'nconst_y'], axis=1)

#Rename columns
df_renamed = df_dropped.rename(columns={"primaryName_x": "Director", 
                                       "primaryName_y": "Writer", 
                                       "averageRating": "Movie_rating",
                                        "startYear": "Movie_release",
                                       "primaryTitle": "Movie_name"})
#Remove general duplicates
df_no_gen_dup = df_renamed.drop_duplicates()

#Remove duplicate movies
df_imdb = df_no_gen_dup.drop_duplicates(subset=['Movie_name', 'Movie_release'])

#Format NaN values and convert release year to float
df_imdb['Movie_release'].replace('\\N', np.nan, inplace=True)
df_imdb['Movie_release'] = df_imdb['Movie_release'].astype(float)

#Print the imdb database
df_imdb.head(1)





# Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:

# 1. Wikipedia movie ID
# 2. Freebase movie ID
# 3. Movie name
# 4. Movie release date
# 5. Movie box office revenue
# 6. Movie runtime
# 7. Movie languages (Freebase ID:name tuples)
# 8. Movie countries (Freebase ID:name tuples)
# 9. Movie genres (Freebase ID:name tuples)

columns_mov = ['Wiki_ID', 'Freebase_ID', 'Movie_name', 'Movie_release', 'Movie_revenue', 'Movie_runtime', 
               'Movie_languages', 'Movie_countries', 'Movie_genres']
df_mov = pd.read_table('data/movie.metadata.tsv', header=None, names=columns_mov)
df_mov.head(1)


# Metadata for 450,669 characters aligned to the movies above, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:

# 1. Wikipedia movie ID
# 2. Freebase movie ID
# 3. Movie release date
# 4. Character name
# 5. Actor date of birth
# 6. Actor gender
# 7. Actor height (in meters)
# 8. Actor ethnicity (Freebase ID)
# 9. Actor name
# 10. Actor age at movie release
# 11. Freebase character/actor map ID
# 12. Freebase character ID
# 13. Freebase actor ID

columns_char = ['Wiki_ID', 'Freebase_ID', 'Movie_release', 'Char_name', 'Actor_birth', 'Actor_gender', 
               'Actor_height', 'Actor_ethnicity', 'Actor_name', 'Actor_age_release', 'Char/Actor_map', 
               'Char_ID', 'Actor_ID']
df_char = pd.read_table('data/character.metadata.tsv', header=None, names=columns_char)
df_char.head(1)


#ANOMALY FOUND IN THE DATABASE, MOVIE RELEASE IN 1010-12-02
df_mo22 = df_mov[df_mov["Movie_name"] == "Hunting Season"]
# Correction
df_mov.loc[df_mov["Movie_name"] == "Hunting Season", "Movie_release"] = "2010-12-02"





df_char.isnull().sum()





# Calculate, sort, and round the missing percentages
missing_percentage = (df_char.isnull().sum() / len(df_char) * 100).sort_values(ascending=False).round(2)

# Convert the Series to DataFrame for compatibility with plotly
df_missing = missing_percentage.reset_index()
df_missing.columns = ['Column', 'Missing Percentage']

# Create the plot with a specified color range
fig = px.bar(df_missing, x='Column', y='Missing Percentage', 
             color='Missing Percentage', 
             color_continuous_scale="RdYlGn_r", 
             range_color=[0, 100],  # Specifying the color scale range
             title='Percentage of Missing Values by Column')
fig.update_layout(coloraxis_colorbar=dict(title="Missing Percentage"))
fig.show()


msno.heatmap(df_char)





# drop records which contain missing values
data_dropped = df_char.dropna()
print("Number of rows after dropping missing records: ", len(data_dropped))








# Calculate, sort, and round the missing percentages
missing_percentage = (df_mov.isna().sum() / len(df_mov) * 100).sort_values(ascending=False).round(2)

# Convert the Series to DataFrame for compatibility with plotly
df_missing = missing_percentage.reset_index()
df_missing.columns = ['Column', 'Missing Percentage']

# Create the plot with a specified color range
fig = px.bar(df_missing, x='Column', y='Missing Percentage', 
             color='Missing Percentage', 
             color_continuous_scale="RdYlGn_r", 
             range_color=[0, 100],  # Specifying the color scale range
             title='Percentage of Missing Values by Column')
fig.update_layout(coloraxis_colorbar=dict(title="Missing Percentage"))
fig.show()


msno.heatmap(df_mov)








# Convert string representation of a dictionary to an actual dictionary
df_mov['Genre_dict'] = df_mov['Movie_genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else {})

# Extract genres
df_mov['Main_genre'] = df_mov['Genre_dict'].apply(lambda x: list(x.values())[0] if len(x) > 0 else None)
df_mov['Sec_Genre'] = df_mov['Genre_dict'].apply(lambda x: list(x.values())[1] if len(x) > 1 else None)

#Copy
df_mov1 = df_mov.copy()

# Drop the temporary Genre_dict column
df_mov1.drop(columns=['Genre_dict'], inplace=True)

df_mov1['Movie_release'] = pd.to_datetime(df_mov1['Movie_release'], errors='coerce')

# Extract year, month, and day into new columns
df_mov1['Year_of_release'] = df_mov1['Movie_release'].dt.year
df_mov1['Month_of_release'] = df_mov1['Movie_release'].dt.month





# Calculate mean revenue for each genre pair
genre_pair_mean_revenue = df_mov1.groupby(['Main_genre', 'Sec_Genre'])['Movie_revenue'].mean().reset_index()

# Merge this mean revenue with the original df_mov1 on 'Main_genre' and 'Sec_Genre'
df_mov1 = df_mov1.merge(genre_pair_mean_revenue, on=['Main_genre', 'Sec_Genre'], suffixes=('', '_mean'))

# Fill NaN values in 'Movie_revenue' with the mean revenue of the genre pairs
df_mov1['Movie_revenue'] = df_mov1['Movie_revenue'].fillna(df_mov1['Movie_revenue_mean'])

# Drop the auxiliary 'Movie_revenue_mean' as it's no longer needed after the NaN values are filled
df_mov1.drop(columns=['Movie_revenue_mean'], inplace=True)

# Drop rows where either 'Main_genre' or 'Secondary_Genre' is NaN
df_genre_pairs = df_mov1.dropna(subset=['Main_genre', 'Sec_Genre'])

# Drop rows where either 'Main_genre' or 'Secondary_Genre' is 'Short Film'
df_genre_pairs = df_genre_pairs[(df_genre_pairs['Main_genre'] != 'Short Film') & (df_genre_pairs['Sec_Genre'] != 'Short Film')]

month_dict = {
    1: "January", 2: "February", 3: "March", 4: "April", 5: "May", 6: "June",
    7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"
}

# Create the 'genre_pairs_aggregated' DataFrame with 'count' and 'mean_revenue' as columns
genre_pairs_aggregated = df_genre_pairs.groupby(['Main_genre', 'Sec_Genre']).agg(
    count=('Wiki_ID', 'size'), 
    mean_revenue=('Movie_revenue', 'mean')
).reset_index()

top_10_genre_pairs = genre_pairs_aggregated[genre_pairs_aggregated['count'] >= 150].sort_values(by='mean_revenue', ascending=False).head(10)

# Sort the top_10_genre_pairs list alphabetically by main genre only# 'top_10_genre_pairs' would be a DataFrame here, you need to extract the genre pairs as a list of tuples
top_10_genre_pairs_list = top_10_genre_pairs[['Main_genre', 'Sec_Genre']].values.tolist()

# Then, sort the list of tuples based on the main genre
top_10_genre_pairs_sorted_by_main_genre = sorted(top_10_genre_pairs_list, key=lambda x: x[0])

# Create an empty subplot
fig = make_subplots(rows=1, cols=1, shared_yaxes=True)

# Loop through each of the top 10 genre pairs
for main_genre, secondary_genre in top_10_genre_pairs_sorted_by_main_genre:
    pair_data = df_mov1[(df_mov1['Main_genre'] == main_genre) & (df_mov1['Sec_Genre'] == secondary_genre)]
    month_mean_revenues = pair_data.groupby('Month_of_release')['Movie_revenue'].mean().reset_index()
    month_mean_revenues['Month_of_release'] = month_mean_revenues['Month_of_release'].replace(month_dict)

    trace = go.Bar(x=month_mean_revenues['Month_of_release'],
                   y=month_mean_revenues['Movie_revenue'],
                   name=f"{main_genre} & {secondary_genre}",
                   opacity=1,
                   marker_color=px.colors.sequential.Plasma_r)

    fig.add_trace(trace)

# Create dropdown menu to switch between the sorted genre pairs
dropdown = []
for i, (main_genre, secondary_genre) in enumerate(top_10_genre_pairs_sorted_by_main_genre):
    # Each trace corresponds to a genre pair
    # For each button, only the corresponding trace should be visible
    visibility = [False] * len(top_10_genre_pairs_sorted_by_main_genre)
    visibility[i] = True  # Only the current trace is visible
    
    option = dict(label=f"{main_genre} & {secondary_genre}",
                  method='update',
                  args=[{'visible': visibility},
                        {'title': f'Mean Box Office Revenues by Month of Release for {main_genre} & {secondary_genre}'}])
    dropdown.append(option)

# Update layout with the sorted dropdown
fig.update_layout(
    updatemenus=[dict(
        type='dropdown',
        showactive=True,
        buttons=dropdown,  # Use the sorted dropdown list here
        direction="down",
        pad={"r": 10, "t": 10},
        x=1.1,
        xanchor="left",
        y=1.2,
        yanchor="top"
    )],
    # ... (keep the rest of your layout configuration as is)
)








df_mov_rel = df_mov.copy()

#Convert the date format into float for merging purposes
df_mov_rel['Movie_release'] = df_mov_rel['Movie_release'].apply(lambda x: float(str(x).split('-')[0]))

#Merge the imdb database with our main movie database
df_movie = df_mov_rel.merge(df_imdb, on=['Movie_release', 'Movie_name'], how='left')
df_movie = df_movie.drop(['Freebase_ID', 'Movie_genres', 'Genre_dict'], axis=1)

#Merge actor database
df_all = df_movie.merge(df_char, on='Wiki_ID', how='right')
df_all = df_all.drop(['Wiki_ID', 'Char/Actor_map', 'Char_ID', 'Actor_ID', 'Movie_release_y'], axis=1)
df_all = df_all.rename(columns={'Movie_release_x': 'Movie_release'})

df_all.head()





#We will focus on the United States of America for our analysis as they contain the most non-nan values 
df_movie = df_all[df_all['Movie_countries'].str.contains("United States of America")]
print(f"The database has now {len(df_movie)} entries.")


df_movie = df_movie.dropna(subset=['Actor_name','Movie_name','Movie_revenue','Movie_release'])
print(f"The database has now {len(df_movie)} entries.")


msno.heatmap(df_movie)





columns_inf = ['year', 'amount','inflation rate']
inflation = pd.read_table('data/inflation_data.csv', header=None, names=columns_inf,sep=',')
inflation = inflation.drop(index=0)

#From https://www.officialdata.org/us/inflation/1888?amount=1

value_in_2023 = [32.39,33.44,33.81,33.81,33.81,34.19,35.78,36.63,36.63,37.07,
                   37.07,37.07,36.63,36.20,35.78,34.96,34.57,34.96,34.19,32.73,
                   33.44,33.81,32.39,32.39,31.72,31.08,30.77,30.46,28.23,24.04,
                   20.38,17.78,15.38,17.19,18.31,17.99,17.99,17.58,17.38,17.68,
                   17.99,17.99,18.42,20.24,22.46,23.67,22.96,22.46,22.13,21.37,
                   21.82,22.13,21.98,20.93,18.88,17.78,17.48,17.09,15.78,13.80,
                   12.77,12.93,12.77,11.83,11.61,11.52,11.44,11.48,11.31,10.95,
                   10.65,10.57,10.39,10.29,10.19,10.05,9.92,9.77,9.50,9.21,8.84,
                   8.38,7.93,7.60,7.36,6.93,6.24,5.72,5.41,5.08,4.72,4.24,3.73,
                   3.38,3.19,3.09,2.96,2.86,2.81,2.71,2.60,2.48,2.35,2.26,2.19,
                   2.13,2.08,2.02,1.96,1.92,1.89,1.85,1.79,1.74,1.71,1.67,1.63,
                   1.58,1.53,1.48,1.43,1.43,1.41,1.37,1.34,1.32,1.30,1.30,1.28,
                   1.26,1.22,1.20,1.19,1.14,1.05,1]

inflation["Inflation Factor for 2023"] = value_in_2023
inflation["year"] = inflation["year"].astype(float)

df_movie['Inflation Factor for 2023'] = df_movie['Movie_release'].map(inflation.set_index('year')['Inflation Factor for 2023'])
df_movie['2023 valued revenue'] = df_movie['Movie_revenue'] * df_movie['Inflation Factor for 2023']

df_mood = df_movie.sort_values(by=['2023 valued revenue'],ascending = False)
df_mood.head(2)





# Group the DataFrame by year and calculate the average revenue for "Non-inflated revenue" and "Movie_revenue"
average_revenue = df_mood.groupby('Movie_release')[['2023 valued revenue', 'Movie_revenue']].mean()

# Extract the years and corresponding average revenue values
years = average_revenue.index
non_inflated_revenue = average_revenue['2023 valued revenue']
movie_revenue = average_revenue['Movie_revenue']

# Create a line plot
plt.figure(figsize=(10, 6))
plt.plot(years, non_inflated_revenue, label='2023 valued revenue', marker='o')
plt.plot(years, movie_revenue, label='Movie_revenue', marker='o')

# Set labels and legend
plt.xlabel('Year')
plt.ylabel('Average Revenue')
plt.title('Average Movie Revenue per Year')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()








def calculate_avg_revenue(row, grouped_data):
    previous_films = grouped_data.get_group(row['Actor_name'])
    previous_films = previous_films[previous_films['Movie_release'] <= row['Movie_release']]
    if previous_films.empty:
        return np.nan
    else:
        return previous_films['Movie_revenue'].mean()
    
def calculate_longevity(row, grouped_data):
    first_film_date = grouped_data.get_group(row['Actor_name'])['Movie_release'].min()
    if (len(grouped_data.get_group(row['Actor_name'])['Movie_release']) == 0) : 
        return 0
    elif first_film_date == np.nan : 
        return np.nan
    else :
        return row['Movie_release'] - first_film_date

def calculate_films_count(row, grouped_data):
    previous_films = grouped_data.get_group(row['Actor_name'])
    return previous_films[previous_films['Movie_release'] <= row['Movie_release']].shape[0]

def calculate_avg_rating(row, grouped_data):
    previous_films = grouped_data.get_group(row['Actor_name'])
    previous_films = previous_films[previous_films['Movie_release'] <= row['Movie_release']]
    if previous_films.empty:
        return np.nan
    else:
        return previous_films['Movie_rating'].mean()


#Create the actors subgroups
grouped_by_actor = df_movie.groupby('Actor_name')

#Create the new columns
df_movie['Avg_revenue_per_film_at_release'] = df_movie.apply(lambda row: calculate_avg_revenue(row, grouped_by_actor), axis=1)
df_movie['Longevity'] = df_movie.apply(lambda row: calculate_longevity(row, grouped_by_actor), axis=1)
df_movie['Number_of_film_at_release'] = df_movie.apply(lambda row: calculate_films_count(row, grouped_by_actor), axis=1)
df_movie['Avg_rating_per_film_at_release'] = df_movie.apply(lambda row: calculate_avg_rating(row, grouped_by_actor), axis=1)

# Ensure that Movie_release is sorted in ascending order for each actor
df_movie = df_movie.sort_values(by=['Actor_name', 'Movie_release'])

# Group by Actor_name and count the films in each group
df_movie['film_count'] = df_movie.groupby('Actor_name').cumcount()

# Add a new column boolean to see if this is the first film of an actor or not
df_movie['First_film'] = df_movie['film_count'] == 0
# Group by Actor_name and get the first release year for each actor
df_movie['first_movie_year'] = df_movie.groupby('Actor_name')['Movie_release'].transform('min')
# Create a new column 'First_film' which is True if 'Movie_release' is the same as 'first_movie_year'
df_movie['First_film'] = df_movie['Movie_release'] == df_movie['first_movie_year']

# Drop the useless columns
df_movie.drop('first_movie_year', axis=1, inplace=True)
df_movie.drop('film_count', axis=1, inplace=True)



df_movie.columns





# Group by 'Movie_name' and aggregate 'Actor_name' into lists
actor_pairs = df_movie.groupby(['Movie_name', 'Movie_release'])['Actor_name'].apply(list)

# For each movie, create all possible pairs of actors without duplication
actor_pairs = actor_pairs.apply(lambda x: list(combinations(sorted(set(x)), 2)))

df_pairs = pd.DataFrame(actor_pairs)
df_pairs = df_pairs.reset_index()
df_pairs = df_pairs.rename(columns={'Actor_name': 'Actor_pairs'})
df_pairs = df_pairs.explode('Actor_pairs')
df_pairs.head()


df_pairs = df_pairs.merge(df_all[['Movie_name', 'Movie_release', 'Movie_revenue', 'Movie_rating']], on=['Movie_name', 'Movie_release'], how='left')


df_pairs.dropna(subset=['Actor_pairs', 'Movie_release', 'Movie_revenue', 'Movie_rating'], inplace=True)


df_pairs['Actor1'] = df_pairs['Actor_pairs'].apply(lambda x: x[0])
df_pairs['Actor2'] = df_pairs['Actor_pairs'].apply(lambda x: x[1])
df_pairs.reset_index(drop=True, inplace=True)


def query_info_for_actor_pairs(row, df, infos):
    actor1 = row['Actor1']
    actor2 = row['Actor2']
    movie_name = row['Movie_name']
    movie_release = row['Movie_release']

    # Create a mask for the condition
    condition_mask = (df['Movie_name'] == movie_name) & (df['Movie_release'] == movie_release)

    # Filter the DataFrame based on the condition
    filtered_df = df[condition_mask]

    # Extract the relevant information for actor1 and actor2
    info1 = filtered_df.loc[filtered_df['Actor_name'] == actor1, infos].values.flatten()
    info2 = filtered_df.loc[filtered_df['Actor_name'] == actor2, infos].values.flatten()

    return info1, info2


def all_infos(row):
    infos1, infos2 = query_info_for_actor_pairs(row, df_movie, ['Actor_age_release', 'Number_of_film_at_release', 'Avg_revenue_per_film_at_release', 'First_film'])

    age_difference = abs(infos1[0] - infos2[0])
    film_count_difference = abs(infos1[1] - infos2[1])
    average_revenue_difference = abs(infos1[2] - infos2[2])
    first_film = (infos1[3] and infos2[3])
    first_film_for_one = (infos1[3] or infos2[3])

    return age_difference, film_count_difference, average_revenue_difference, first_film, first_film_for_one


def number_of_films_together(row, df):
    actor1 = row['Actor1']
    actor2 = row['Actor2']
    movie_release = row['Movie_release']

    condition_mask1 = ((df['Movie_release'] < movie_release) & (df['Actor_name'] == actor1))
    condition_mask2 = ((df['Movie_release'] < movie_release) & (df['Actor_name'] == actor2))

    movie_name1 = df.loc[condition_mask1, 'Movie_name'].values.tolist()
    movie_name2 = df.loc[condition_mask2, 'Movie_name'].values.tolist()

    return len(set(movie_name1).intersection(set(movie_name2)))



infos = df_pairs.apply(lambda x: all_infos(x), axis=1)


df_pairs[['Age_difference', 'Film_count_difference', 'Average_revenue_difference', 'First_film', 'First_film_for_one']] = pd.DataFrame(infos.tolist(), index=df_pairs.index)


films_together = df_pairs.apply(lambda x: number_of_films_together(x, df_movie), axis=1)


df_pairs['Number_of_films_together'] = films_together





# Group by 'Movie_name' and aggregate 'Actor_name' into lists
actor_pairs = df_movie.groupby('Movie_name')['Actor_name'].apply(list)

# For each movie, create all possible pairs of actors without duplication
actor_pairs = actor_pairs.apply(lambda x: list(combinations(sorted(set(x)), 2)))

# Flatten the list of actor pairs into a new dataframe
actor_pair_list = [pair for sublist in actor_pairs for pair in sublist]
df_actor_pairs = pd.DataFrame(actor_pair_list, columns=['Actor_1', 'Actor_2'])

# Count the occurrences of each pair
pair_counts = df_actor_pairs.value_counts().reset_index(name='Occurrences')

# We restrict our pairs to have at least 3 films together 
pairs_more_than_three = pair_counts[pair_counts['Occurrences'] >= 3]

pairs_more_than_three.head(2)





# Initialize PyVis Network
net = Network(notebook=True, 
              cdn_resources="remote", 
              bgcolor="#222222", 
              font_color="white", 
              height="500px",
              select_menu=True,
              filter_menu=True)

net.set_options("""
const options = {
  "physics": {
    "forceAtlas2Based": {
      "gravitationalConstant": -84,
      "centralGravity": 0.09,
      "springLength": 20,
      "springConstant": 0.035
    },
    "minVelocity": 0.18,
    "solver": "forceAtlas2Based"
  }
}
""")

# Combine 'Actor_1' and 'Actor_2' into a single Series and count the occurrences of each actor
actor_pairings = pd.concat([pairs_more_than_three['Actor_1'], pairs_more_than_three['Actor_2']]).value_counts()

# Add nodes to the network with the scaled size
for actor, size in actor_pairings.items():
    net.add_node(actor, size=size+5)
    
# Create edges with weights (the number of occurrences as value)
edges = pairs_more_than_three.apply(lambda row: (row['Actor_1'], row['Actor_2'], 
                                                row['Occurrences']), axis=1).tolist()

# Add weighted edges to the network
for edge in edges:
    net.add_edge(edge[0], edge[1], value=edge[2])

# Save or show the network
net.show("edges.html")








#Function used for one-hot encoding of different values
def contains_(str_, contenant):
    try:
        dict_ = ast.literal_eval(str_)
        return contenant in dict_.values()
    except (ValueError, SyntaxError):
        # In case the string is not a proper dictionary format, we return False
        return False
    
def z_score(df, columns):
    df_z = df.copy()
    for item in columns:
        df_z[item] = (df_z[item] - df_z[item].mean())/(df_z[item].std() + 1e-30)
    
    return df_z


#df_usa is simply the data that we have at the end of the section 2.Merging the data
df_usa = pd.read_table('data/data_usa.tsv')
df_usa.head(1)


#Here we retrieve the 6 most common languages present in the films in the database 
#We one-hot encode them and use them for the linear regression
str_languages = df_usa['Movie_languages'].apply(ast.literal_eval).tolist()
lang = [ [ i for i in item.values() ] for item in str_languages ]
lang_flat = [element for sublist in lang for element in sublist]

lang_counts = Counter(lang_flat)
top_lang = lang_counts.most_common(6)


languages = ['English Language', 'Spanish Language', 'French Language', 
             'German Language', 'Italian Language', 'Russian Language']
    
for i in range(len(languages)):
    language = languages[i]
    df_usa[language] = df_usa['Movie_languages'].apply(lambda x: contains_(x, language))


#Here we retrieve the 10 most popular movie genres present in the films in the database 
#We one-hot encode them and use them for the linear regression
str_genres = df_usa['Movie_genres'].apply(ast.literal_eval).tolist()
genres = [ [ i for i in item.values() ] for item in str_genres ]
genres_flat = [element for sublist in genres for element in sublist]

genre_counts = Counter(genres_flat)
top_genres = genre_counts.most_common(10)
genres = ['Drama', 'Comedy', 'Thriller', 'Romance Film', 'Action', 'Adventure', 'Crime Fiction', 
             'Family Film', 'Indie']
    
for i in range(len(genres)):
    genre = genres[i]
    df_usa[genre] = df_usa['Movie_genres'].apply(lambda x: contains_(x, genre))


#Here we define the actor-pairs database
df_usa['Actor_name'] = df_usa['Actor_name'].astype(str)

# Group by 'Movie_name' and aggregate 'Actor_name' into lists
actor_pairs = df_usa.groupby('Movie_name')['Actor_name'].apply(list)

# For each movie, create all possible pairs of actors without duplication
actor_pairs = actor_pairs.apply(lambda x: list(combinations(sorted(set(x)), 2)))

# Flatten the list of actor pairs into a new dataframe and associate with the movie name
actor_pair_list = [(movie, *pair) for movie, pairs in actor_pairs.items() for pair in pairs]
df_actor_pairs = pd.DataFrame(actor_pair_list, columns=['Movie_name', 'Actor_1', 'Actor_2'])


#Merge on the actor pairs on the movie dataset
df = df_actor_pairs.merge(df_usa, on='Movie_name', how='inner')
df = df.drop_duplicates(subset=['Movie_name', 'Actor_1', 'Actor_2'])
df = df.drop(['Movie_languages', 'Movie_countries', 'Movie_genres', 'Movie_producer', 
             'Movie_composer', 'Movie_director', 'Movie_writer', 'Char_name', 'Actor_age_release', 
             'Actor_ethnicity', 'Actor_name', 'Actor_birth', 'Actor_height', 'Actor_gender'], axis=1)


#Normalize continuous features
df = z_score(df, ['Movie_release', 'Movie_revenue', 'Movie_rating'])
df = df.dropna()
print(f"The database has now {len(df)} entries")
df = df.rename(columns={'English Language': 'eng', 'Spanish Language': 'spain', 'French Language': 'fr', 
                       'German Language': 'ger', 'Italian Language': 'ita', 'Russian Language': 'rus', 
                       'Romance Film': 'Romance', 'Crime Fiction': 'Crime_fiction', 
                       'Family Film': 'Family'})


#Linear regression
mod = smf.ols(formula="Movie_revenue ~  Movie_release + Movie_rating + C(eng) \
              + C(spain) + C(fr) + C(ger) + C(ita) + C(rus) + C(Drama) + C(Comedy) \
              + C(Thriller) + C(Romance) + C(Action) + C(Adventure) + C(Crime_fiction) \
              + C(Family) + C(Indie)", data=df)

res = mod.fit()

# Extract the estimated propensity scores
df['Propensity_score'] = res.predict()

print(res.summary())
